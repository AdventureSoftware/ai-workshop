# Workshop Exercises

These exercises are designed to practice AI pair-programming techniques from the presentation. Each exercise builds on the concepts learned and provides hands-on experience with GitHub Copilot.

## Exercise 1: FCE Pattern Practice (20 minutes)

**Goal**: Master the Function-Constraints-Examples pattern

**Task**: Complete the payment processor in `payment-processor.ts`

**Instructions**:
1. Open `payment-processor.ts`
2. Use the FCE pattern to prompt Copilot for each function
3. Include specific constraints (amounts, currencies, error cases)
4. Provide concrete input/output examples
5. Apply the R.E.D. checklist to verify results

**Success Criteria**:
- [ ] All functions implemented with proper error handling
- [ ] Business rules from API_SCHEMA.md followed
- [ ] Tests pass
- [ ] Code follows STYLE_GUIDE.md patterns

## Exercise 2: Context Injection (25 minutes)

**Goal**: Learn to provide rich context for better AI suggestions

**Task**: Implement user management system in `user-service.ts`

**Instructions**:
1. Open related files: `types/auth.ts`, `API_SCHEMA.md`, `STYLE_GUIDE.md`
2. Keep these files open while working (Copilot uses open tabs as context)
3. Use inline comments to provide intent
4. Reference types with # syntax in Copilot Chat
5. Use the documentation-driven development approach

**Success Criteria**:
- [ ] Registration function with full validation
- [ ] Authentication with rate limiting
- [ ] Password hashing with bcrypt
- [ ] JWT token generation
- [ ] All edge cases handled

## Exercise 3: Test-First Development (20 minutes)

**Goal**: Use tests as specifications for AI code generation

**Task**: Build shipping calculator using test-first approach

**Instructions**:
1. Start by writing comprehensive tests in `shipping.test.ts`
2. Include edge cases and business rules in test names
3. Prompt Copilot: "Implement shipping calculator to pass these tests"
4. Use Copilot `/tests` command to generate additional test cases
5. Apply R.E.D. verification process

**Success Criteria**:
- [ ] Tests written first, covering happy path and edge cases
- [ ] Implementation generated by AI passes all tests
- [ ] Business logic from API_SCHEMA.md correctly implemented
- [ ] Performance requirements met

## Exercise 4: Security Verification (15 minutes)

**Goal**: Practice identifying and fixing security issues in AI-generated code

**Task**: Review and secure the authentication system

**Instructions**:
1. Use Copilot to generate login/registration functions
2. Apply security review checklist
3. Look for common vulnerabilities (injection, weak validation, etc.)
4. Use prompt: "Add input validation & safe defaults. Flag any potential security issues"
5. Implement fixes for identified issues

**Success Criteria**:
- [ ] Input validation on all parameters
- [ ] SQL injection prevention
- [ ] Rate limiting implemented
- [ ] Proper error handling (don't leak sensitive info)
- [ ] Security tests added

## Exercise 5: Advanced Context Strategies (25 minutes)

**Goal**: Master multi-file refactoring and complex context management

**Task**: Refactor the entire system to use dependency injection

**Instructions**:
1. Open all related files simultaneously
2. Use Copilot Agent Mode (if available) for multi-file changes
3. Use Chat with specific file references: "Refactor #UserService to use #DatabaseInterface"
4. Create interfaces and implement dependency injection pattern
5. Update tests to use mocks

**Success Criteria**:
- [ ] Clean separation of concerns
- [ ] Testable code with dependency injection
- [ ] Interface-based design
- [ ] All tests updated and passing
- [ ] No breaking changes to public APIs

## Exercise 6: Prompt Iteration Practice (20 minutes)

**Goal**: Learn to refine prompts when AI suggestions fail

**Task**: Build a complex validation system with custom business rules

**Instructions**:
1. Start with a vague prompt and observe poor results
2. Iteratively improve prompts using FCE pattern
3. Add concrete examples when AI misunderstands requirements
4. Document the prompt evolution process
5. Compare final results with initial attempt

**Success Criteria**:
- [ ] Document at least 3 prompt iterations
- [ ] Show clear improvement in AI output quality
- [ ] Final implementation meets all requirements
- [ ] Explanation of what made the final prompt successful

## Bonus Challenge: Team Standards Implementation (15 minutes)

**Goal**: Create team-specific prompting guidelines

**Task**: Enhance the STYLE_GUIDE.md with AI-specific guidelines

**Instructions**:
1. Based on your exercise experience, identify common issues
2. Create a "AI Prompting Best Practices" section
3. Add examples of good/bad prompts for your domain
4. Include a prompt template library
5. Test the guidelines with a new feature implementation

## Tips for Success

1. **Open related files**: Copilot uses open tabs as context
2. **Be specific**: Vague prompts lead to generic solutions
3. **Include edge cases**: AI tends to focus on happy paths
4. **Verify everything**: Apply R.E.D. checklist to all AI code
5. **Iterate prompts**: First attempt rarely produces perfect code
6. **Use existing patterns**: Reference your codebase style in prompts

## Common Mistakes to Avoid

- ‚ùå Accepting first AI suggestion without review
- ‚ùå Providing insufficient context in prompts
- ‚ùå Forgetting to test edge cases
- ‚ùå Not following established code patterns
- ‚ùå Skipping security validation
- ‚ùå Working with closed files (AI can't see them)

## Measuring Success

Track your progress:
- Time to implement features (should decrease)
- Number of prompt iterations needed (should decrease)
- Test coverage (should maintain or improve)
- Code review feedback (should focus more on architecture, less on syntax)

Good luck! üöÄ
