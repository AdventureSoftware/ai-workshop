AI in Software Development
Valentin Zuld - 23 Iulie 2025

Efficient AI Pair-Programming Strategies
Goals
â— Master context-rich prompting for accurate, style-aligned suggestions
â— Embed Copilot in coding, testing & PR review without slowing flow
â— Apply the RED checklist (Read â†’ Execute tests â†’ Diff-review) to ship
safe code
Agenda
0 : 00 Intro & Why AI Matters (20 min)

0 : 20 Copilot Toolbox â€“ live tour (30 min)

0 : 50 Prompt Patterns + micro-exercises (40 min)

1 : 30 Break #1 (10 min)

1 : 40 Injecting Context Like a Pro (40 min)

2 : 20 Verification, Security & Team Practices (30 min)

2 : 50 Measuring Success & KPIs (10 min)

3 : 00 Break #2 (10 min)

3 : 10 Q&A + Hands-On Lab (50 min)

4 : 00 Wrap-Up

AI is Reshaping Dev Work
ğŸ“Š Impact Metric (2024-25)

â— 70 % of Copilot early adopters say
theyâ€™re more productive - source:
Microsoft
â— Controlled study: users finished
coding/search/summarize tasks 29 %
faster with Copilot - source: Microsoft
â— 81 % of devs list â€œincrease productivityâ€
as the #1 benefit of AI tools (Stack
Overflow â€™24)
https://survey.stackoverflow.co/2024/ai
Impact by Experience Level:
Gains vs. Trade-offs
ğŸš€ Gains âš  Watch-outs
Juniors
Instant explanations & examples
Faster prototypes & POCs
Exposure to idiomatic code
Shallow grasp of fundamentals
Higher risk of accepting hallucinations
May skip debugger practice
Seniors â€¢ 22 % faster coding on routine
tasks (Jellyfish, 2025)
Offload boilerplate, focus on
architecture
Rapid test-scaffold generation
Can be 19 % slower if mis-prompting
(METR RCT, Jul 2025)
Extra review load for AI code
Risk of â€œrubber-stampâ€ oversight
AI Feedback Loop
Real-World AI-Assisted Projects
PRESENTER
Real-World AI-Assisted Projects
SPEECH TO
SIGN LANGUAGE
Real-World AI-Assisted Projects
Inventory
Machine
Dashboard
Supervisor Maintenance

Todayâ€™s Focus
Copilotâ€™s super-power = the context you feed it

â— 3 prompt recipes (FCE, Edge-Case Booster, Test-First)
â— 4 context levers (comments, neighbours, workspace, docs)
Copilot Toolbox
Inline Suggestions â€“ real-time completions as you type
Copilot Chat â€“ ask â€œ/explainâ€, â€œ/testsâ€, â€œ/fixâ€ right inside the IDE
Copilot Agent Mode (preview) â€“ multi-file refactors & test generation
PR / Code Review â€“ request Copilot as a reviewer on GitHub.com
Copilot Spaces â€“ curate docs & specs so Copilot answers with domain
knowledge (public preview)
Inline Suggestions
// âœ… Good context
// Calculate shipping cost based on weight and distance
function calculateShipping(weight, distance) {
// Copilot gets clear intent

// âŒ Vague context
function calc(w, d) {
// Copilot has to guess

Essential Shortcuts
â— Accept : Tab (full suggestion) or Ctrl+â†’ (word by word)
â— Navigate : Alt+] (next) / Alt+[ (previous suggestion)
â— Reject : Esc or keep typing to override
â— Partial accept : Highlight good part â†’ Tab â†’ discard rest
Copilot Chat
Essential Slash Commands

â— /explain - Decode complex code, algorithms, or regex patterns
â— /tests - Generate unit tests with edge cases and mocks
â— /fix - Debug failing tests or runtime errors
â— /optimize - Improve performance, reduce complexity
Advanced Chat Techniques

Reference specific code
"Refactor the #calculateDiscount function to use #PricingStrategy pattern"

Multi-step conversations

"/explain this authentication flow"
"Now add rate limiting to step 3"
"Generate tests for the rate limiter"
Context-aware requests

"Convert this REST endpoint to GraphQL, keeping the same validation"
Copilot Chat
Smart Context References

â— #filename - Reference specific files
â— #functionName - Target exact functions/classes
â— Selection-aware - Automatically uses highlighted code as context
Beyond Code Generation

â— Documentation : "Write API docs for this controller"
â— Code review : "What could go wrong with this implementation?"
â— Learning : "Explain the trade-offs between these two approaches"
Agent Mode
What Agent Mode Does

â— Multi-file operations : Refactor across multiple files simultaneously
â— Context-aware changes : Understands file relationships and
dependencies
â— Intelligent planning : Breaks down complex tasks into steps
â— Test generation : Creates comprehensive test suites automatically
When to Use Agent Mode

â— Large refactoring across multiple components
â— Adding new features that touch many files
â— Generating test coverage for existing code
â— API redesigns with multiple endpoints
Agent Mode
Prompt: "Add user role-based permissions to the entire auth system"

Agent Mode:

Analyzes auth-related files
Updates User model + migration
Modifies AuthController methods
Updates middleware functions
Generates comprehensive tests
Updates API documentation
Best Practices
â— Start with clear, specific goals
â— Review each file change before accepting
â— Test thoroughly - agent mode can introduce subtle bugs
â— Current status : Preview feature (VS Code extension)
Copilot Spaces - Your Team's AI
Knowledge Base
What Spaces Solve
â— Copilot knows Stack Overflow but not your company's APIs
â— Domain knowledge scattered across wikis, Confluence, Slack
â— Repeated explanations of internal systems to AI
How Spaces Work
â— Upload team docs, API schemas, coding standards
â— Copilot references YOUR content when generating suggestions
â— Persistent context across all conversations
Copilot Spaces - Your Team's AI
Knowledge Base
Example Setup
â— /Backend-Team-Space/
â— â”œâ”€â”€ api-guidelines.md
â— â”œâ”€â”€ database-schema.sql
â— â”œâ”€â”€ error-handling-patterns.js
â— â”œâ”€â”€ deployment-checklist.md
â— â””â”€â”€ business-rules/
â— â”œâ”€â”€ pricing-logic.md
â— â””â”€â”€ user-permissions.md
Tips:
â— Include real code examples from your codebase
â— Add "gotchas" documentation - common mistakes to avoid
â— Update when major changes happen
â— Current status: Public preview (GitHub Copilot Enterprise)

What Copilot Actually Sees
Context Signals (ranked)

Current file (full buffer)
Open tabs / neighbours
Symbols in workspace index
Additional artefacts: specs, tests, docs, Spaces
Implication: Feed domain rules & style near your cursor or via a Space for best
accuracy.

Reminder: â€œGarbage context = garbage suggestions.â€

Demo Time
Why Prompt Engineering Matters
Point: Time saved comes from clarity, not magic.

The Reality: AI Follows Instructions Literally
âŒ Vague: "Create a login function"
Result: Basic username/password, no validation, hardcoded responses

âœ… Specific: "Create secure login with email validation, bcrypt hashing,
rate limiting ( 5 attempts/min), JWT response, proper error handling"

Result: Production-ready authentication with security best practices

Why Prompt Engineering Matters
Real Impact on Development Speed

â— Bad prompts â†’ 3-4 iterations â†’ 20+ minutes for simple function
â— Good prompts â†’ Working code first try â†’ 2-3 minutes total
â— Compound effect â†’ 2+ hours saved per day across team
The "Garbage In = Garbage Out" Problem

â— AI amplifies your input quality
â— Unclear requirements â†’ Unclear code
â— Missing constraints â†’ Security vulnerabilities
â— No examples â†’ Wrong assumptions about data format
Prompt Engineering = Better Code Reviews

â— Specific prompts lead to self-documenting code
â— Clear constraints reduce back-and-forth in PRs
â— Examples in prompts become comments in code
Mental Model:
Prompt â†’ Context Funnel â†’ Prediction
Higher up the funnel = bigger impact on suggestion
quality.
Copilot never sees closed files; bring context to the
model.
Recipe 1: FCE Pattern
â— Include error examples: "Invalid email â†’ {valid: false, error: 'Invalid format'}"
â— Use real data formats from your system

Function : Gives AI clear intent and naming context
Constraints : Prevents hallucinations and edge case bugs
Examples : Shows exact data formats and business logic
âŒ "Create a payment processor"
âœ… "Function: Process credit card payments via Stripe
Constraints: USD only, $5-$10k limits, retry failed payments 2x
Examples: {amount: 2999, currency: 'USD'} â†’ {success: true, id: 'ch_123'}"
Recipe 2: Edge-Case Booster
Why it works: Explicitly calls out failure modes AI should consider

The Problem: AI Loves Happy Paths

â— Default AI behavior: assumes perfect input, network, and conditions
â— Real world: null values, network timeouts, malformed data, race conditions
â— Result: Code that works in demos but breaks in production
Edge-Case Booster Template

// Handle user authentication with edge cases:
// - null/undefined inputs (return early with error)
// - expired/malformed tokens (throw AuthError)
// - rate limiting exceeded (429 status)
// - network failures (retry 3x with backoff)
// - database connection lost (graceful degradation)

async function authenticateUser(token) {
Test-First Specification
The Power of Tests as Specifications

â— Tests become executable requirements that AI can follow
â— Eliminates ambiguity about expected behavior
â— Forces you to think through edge cases upfront
â— Built-in verification when AI generates the implementation
// Should validate email format, reject typos, handle international domains
describe('Email validator', () => {
it('accepts valid emails: user@domain.com, test+tag@example.org')
it('rejects invalid: missing@.com, double@@domain.com, spaces in email')
it('handles edge cases: unicode domains, 64+ char local parts')
it('normalizes input: trims whitespace, converts to lowercase')
})

// Now prompt Copilot: "Implement the validateEmail function for these tests"

Common Prompt Failures
â— Vague intent (â€œoptimize codeâ€)

â— Missing constraints (perf, side-effects, API limits)

â— No examples â†’ model guesses format

â— Oversized prompt (>200 lines) â†’ truncation / loss of context

â— Asking multiple distinct tasks in one shot

â— Assuming AI knows your codebase

What â€œContextâ€ Really Means
Everything the model receives before predicting next tokens

â— Current file buffer
â— Open/neighbor files
â— Workspace index (symbols, paths)
â— Extra artefacts: style guides, schemas, docs
Better context â‡’ fewer hallucinations, on-style code

The Four Context Levers
Inline comments / doc-strings - Feature intent, constraints
Open tabs & neighbor files - Interfaces, config, tests
Workspace index - Cross-file calls, types
External docs & style files - .editorconfig, STYLE_GUIDE.md, schema.md
Inline Comments & Docstrings
â— Place intent right above cursor for best weight

â— Keep it concise < 20 tokens so model doesnâ€™t trim deeper
context

Open Tabs & Neighbor Files
â— Copilot looks at all open buffers â®• treat tabs as â€œprompt boostersâ€

â— Open interface / test / schema files before prompting

â— VS Code shortcut: Ctrl + K O to quickly open symbolâ€™s file

Workspace Index & Symbols
â— VS Code & GitHub create a searchable index of your repo

â— Chat supports #-mentions: #calculate_discount, #controllers/OrderController

â— Large monorepo? Enable local index for privacy + speed (settings â†’ Copilot)

External Docs & Style Files
â— .editorconfig â€“ Copilot respects indent, max-line, quote style

â— STYLE_GUIDE.md â€“ add naming conventions â†’ fewer review nits

â— Domain artefacts â€“ schema.md, api.yml, Confluence export

Multi-File Context Strategies
â— Open key files - interfaces, main classes, tests
â— Use Chat with #-mentions - Reference specific functions
â— Work in small chunks - Refactor one class/module at a time
â— Keep related files open - Maintain context between changes
Pro Tips

â— Use "Split Editor" to keep context visible
â— Copilot Chat: "Refactor #UserService to use #DatabaseInterface"
â— Create temporary "REFACTOR_NOTES.md" with context
Documentation-Driven
Development with AI
Write Docs First, Code Second

User Authentication Service
Requirements
Support OAuth2 + API keys
Rate limiting: 100 req/min per user
Audit log all auth events
API Design
POST /auth/login -> {token, expires_at}

Error Handling
429 for rate limits
401 for invalid credentials
Then Prompt Copilot

// Implement the UserAuth service per DOCS.md above
// Use JWT tokens, Redis for rate limiting

class UserAuthService {
Quick-Win Stats
Action Accuracy Gain*
Open neighbor test file +22 % correct first-try suggestions
Add .editorconfig â€“65 % manual formatting fixes reported by
VS Code lint logs
Provide schema.md 3Ã— fewer SQL type errors in staging
pipeline (internal sample)
*Internal & GitHub docs measurements 2023-25.
Donâ€™t Trust Blindly
Reality check

â— LLMs can invent non-existent packages â†’ â€œslopâ€ risk (20 % of code samples)
â— Copilot will happily complete syntactically good but logically wrong code
â— Every suggestion is a hypothesis until you test it.
Verification Checklist
R.E.D. - Your AI Code Safety Net

â— Read : Scan the diff â†’ does it match intent & style?
â— Execute : Run the full test suite, not just new tests
â— Diff-review : Compare against your mental model
Read - Code Review Questions

âœ… Does this solve the actual problem I described?

âœ… Are variable names consistent with our codebase?

âœ… Any magic numbers or hardcoded values that should be constants?

âœ… Does error handling match our team patterns?

âœ… Are there any TODO comments or incomplete sections?

When Copilot Suggestions Fail
Common Failure Patterns

â— Wrong API usage - Check official docs vs. Copilot suggestion
â— Logic errors - AI follows pattern but misses business rules
â— Performance issues - AI optimizes for readability, not speed
â— Integration bugs - Doesn't understand your specific environment
Debug Strategy

Isolate the AI-generated portion
Add logging/breakpoints to see actual vs. expected behavior
Compare with working examples in your codebase
Iterating on Failed Prompts
Prompt Refinement Process

âŒ First try: "Create user validation"
âŒ Second try: "Validate user input with error handling"
âœ… Third try: "Validate user registration form:

Email: RFC 5322 format
Password: 8+ chars, 1 symbol, 1 number
Return: {valid: boolean, errors: string[]}"
Iteration Techniques

â— Add concrete examples of input/output
â— Specify error conditions explicitly
â— Reference existing code patterns in your project
â— Use FCE pattern consistently
Security First:
Secrets & Vulnerabilities
â— Copilot can surface real hard-coded secrets from public repos (GitGuardian)
â— 23 % of Copilot snippets in a CWE-25 audit were insecure by default (Source)
Mitigations

â— Secret scanning (GitGuardian, native GitHub).
â— SAST / CodeQL in CI.
â— Prompt Copilot:
â€œAdd input validation & safe defaults. Flag any potential CWE-â€
Building Safety Nets for AI Code
CI/CD Pipeline Safeguards

â— Pre-commit hooks : ESLint, Prettier, secret detection before code leaves local
â— Pull request gates : Automated security scans block merge until clean
â— Dependency scanning : Flag vulnerable packages AI might suggest
â— Code coverage requirements : Ensure AI-generated code includes tests
Team Process Integration

â— Mandatory security review for AI code touching auth/payments
â— Automated alerts when AI suggests deprecated APIs
â— Quality gates : Block deployment if coverage drops below threshold
Code Review Best Practices
with AI
New Review Questions

â— âœ… "Does this match the original prompt intent?"
â— âœ… "Are edge cases properly handled?"
â— âœ… "Any non-existent packages or APIs?"
â— âœ… "Security: input validation, safe defaults?"
Review Process Updates

â— Tag AI-generated code in PR descriptions
â— Require tests for all AI suggestions
â— Senior dev approval for critical path changes
Team Standards for AI-Generated
Code
Establish Team Guidelines

Team AI Code Standards
Always run tests before committing AI code
Use FCE pattern for complex functions
Required: peer review for AI database/security code
Shared prompt library in team docs
Quality Gates

â— Same standards as human code (linting, coverage, performance)
â— Extra scrutiny for authentication, data handling, API integrations
Personal Productivity KPIs
Track Your Own AI Impact

â— Feature velocity : Stories completed per sprint (before/after
Copilot)
â— Focus time : Hours spent on creative vs. repetitive coding
â— Learning curve : Time to implement unfamiliar
APIs/frameworks
â— Code review feedback : Reduction in style/syntax comments
Weekly Self-Assessment Questions

â— Did I spend less time on boilerplate this week?
â— Am I tackling more complex problems than before?
â— How much time did I save on test writing?
â— Did AI help me learn new patterns/libraries?
Team Impact Metrics
Collective Benefits to Track

â— Sprint burndown improvement : More consistent velocity
â— Knowledge sharing : Junior devs ramping up faster
â— Code consistency : Fewer style debates in PRs
â— Technical debt : More time for refactoring/cleanup
Team Health Indicators

â— Reduced overtime during crunch periods
â— Faster onboarding for new team members
â— More time for architecture discussions vs. syntax fixes
â— Increased participation in code reviews (less tedious work)
Career Development Wins
How AI Makes You a Better Developer

â— Learn faster : Exposure to new patterns and best practices
â— Focus on design : Less time debugging syntax, more on architecture
â— Broader skill set : AI helps you work outside comfort zone
â— Mentoring ability : Teach AI techniques to colleagues
Professional Growth Metrics

â— New technologies adopted this quarter
â— Complex problems solved vs. routine tasks
â— Leadership opportunities (teaching AI practices)
â— Innovation time (freed up from mundane coding)